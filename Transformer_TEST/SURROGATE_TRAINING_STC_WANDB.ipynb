{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igqIMEgu64-F"
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xybQNYCXYu13"
   },
   "outputs": [],
   "source": [
    "# Reading/Writing Data\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Pytorch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import gradcheck\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Self-Defined Package\n",
    "from LSMDataset import LSMDataset\n",
    "from LSMTransformer import LSMLSTM\n",
    "\n",
    "# 忽略 ParserWarning 警告\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.ParserWarning)\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pgkOh2e9UjE"
   },
   "source": [
    "# Configurations\n",
    "`config` contains hyper-parameters for training and the path to save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QoWPUahCtoT6"
   },
   "outputs": [],
   "source": [
    "constant_config = {\n",
    "    'seed': 11611801,      \n",
    "    'test_ratio': 0.2,\n",
    "    'valid_ratio': 0.2,   \n",
    "    'n_epochs': 1000,                \n",
    "    'train_batch_size': 128, \n",
    "    'valid_batch_size': 128,\n",
    "    'test_batch_size': 256,\n",
    "    \n",
    "    'learning_rate': 35e-4,\n",
    "    'step_size': 20,\n",
    "    'gamma': 0.2,\n",
    "    'weight_decay': 0.0025,\n",
    "    'warm_step': 11,\n",
    "    'early_stop': 50, \n",
    "    'hidden_size': 64,\n",
    "    'loss_decrease_threshold': 1e-4,\n",
    "    \n",
    "    # 目前只训练near sites的cmfd点\n",
    "    'soil_layer_num': 8,\n",
    "    'near_sites': [1, 2, 3, 4, 5, 6, 7, 8, 9],         # 文件夹下有LSM跑出来的数据和气象站点的数据\n",
    "    'var_param_list': ['MAXSMC'],\n",
    "    'seq_length': 365,\n",
    "    'peroid': [2010, 2015], # 右开区间\n",
    "    'output': 0,\n",
    "    'root': 'YOUR_ROOT\\\\Transformer_TEST',\n",
    "    'model_save_dir': 'YOUR_ROOT\\\\Transformer_TEST\\\\SURROGATE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "sweep_config['parameters'] = {}\n",
    "\n",
    "# 常量型超参数\n",
    "sweep_config['parameters'].update({\n",
    "    'valid_batch_size': {'value': 900},\n",
    "    'test_batch_size': {'value': 720},\n",
    "    'seed': {'value': 11611801},      \n",
    "    'test_ratio': {'value': 0.2}, \n",
    "    'valid_ratio': {'value': 0.2}, \n",
    "    'n_epochs': {'value': 2000},\n",
    "    'soil_layer_num': {'value': 8},\n",
    "    'seq_length': {'value': 365},\n",
    "    'output': {'value': 0},\n",
    "    'near_sites': {'value': [1, 2, 3, 4, 5, 6, 7, 8, 9]},         # 文件夹下有LSM跑出来的数据和气象站点的数据\n",
    "    'var_param_list': {'value': ['MAXSMC']},\n",
    "    'peroid':{'value':  [2010, 2015]}, # 右开区间\n",
    "    'root': {'value':'YOUR_ROOT\\\\Transformer_TEST'},\n",
    "    'model_save_dir': {'value':'YOUR_ROOT\\\\Transformer_TEST\\\\SURROGATE'},\n",
    "})\n",
    "    \n",
    "# 离散型超参数\n",
    "sweep_config['parameters'].update({\n",
    "    'step_size': {\n",
    "        'values': [10, 20, 40]\n",
    "    },\n",
    "    'train_batch_size': {\n",
    "        'values': [64, 128, 256]\n",
    "    },\n",
    "    'early_stop': {\n",
    "        'values': [50]\n",
    "    },\n",
    "    'hidden_size': {\n",
    "        'values': [32, 64]\n",
    "    },\n",
    "})\n",
    "\n",
    "    \n",
    "# 连续型超参数\n",
    "sweep_config['parameters'].update({\n",
    "    'learning_rate': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-6,\n",
    "        'max': 4e-3\n",
    "      },\n",
    "    'weight_decay': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-4,\n",
    "        'max': 1e-2\n",
    "      },\n",
    "    'gamma': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 5e-1,\n",
    "        'max': 8e-1,\n",
    "      },\n",
    "    'loss_decrease_threshold': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-4,\n",
    "        'max': 1e-3,\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTAVqRfc2KK3"
   },
   "source": [
    "# Some Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RbrcpfYN2I-H"
   },
   "outputs": [],
   "source": [
    "def same_seed(seed): \n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrS-aJJh9XkW"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(config):    \n",
    "    # 创建数据集\n",
    "    dataset = LSMDataset(config)\n",
    "    \n",
    "    # 计算训练集和测试集的长度\n",
    "    train_len = int(len(dataset) * (1-config['test_ratio']))\n",
    "    test_len = len(dataset) - train_len\n",
    "\n",
    "    # 使用 random_split 函数进行划分\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_len, test_len])\n",
    "\n",
    "    # 计算训练集和验证集的长度\n",
    "    valid_len = int(train_len * (config['valid_ratio']))\n",
    "    train_len = train_len - valid_len\n",
    "\n",
    "    # 使用 random_split 函数进行划分\n",
    "    train_dataset, valid_dataset = random_split(train_dataset, [train_len, valid_len])\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['train_batch_size'], drop_last=True, shuffle=True, pin_memory=False)#, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=config['valid_batch_size'], drop_last=False, shuffle=True, pin_memory=False)#, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['test_batch_size'], drop_last=False, shuffle=False)#, collate_fn=collate_fn)\n",
    "\n",
    "    print(len(dataset))\n",
    "    print('training size', len(train_dataset))\n",
    "    print('validing size', len(valid_dataset))\n",
    "    print('testing size', len(test_dataset))\n",
    "    \n",
    "    return dataset, train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_dataloader(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer and Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_loader, valid_loader, model, wandb, device):\n",
    "    # 初始化config和criterion\n",
    "    config = wandb.config\n",
    "    criterion =  nn.MSELoss() \n",
    "\n",
    "    #初始化optimizer和lr_scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay']) \n",
    "    schedulerRP = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=config['step_size'], factor=config['gamma'], min_lr=1e-8, threshold=config['loss_decrease_threshold'])\n",
    "#     schedulerEXP = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config['gamma'])\n",
    "#     schedulerSTEP = torch.optim.lr_scheduler.StepLR(optimizer, gamma=config['gamma'], step_size=config['step_size'])\n",
    "      \n",
    "    # table = wandb.Table(columns=[\"epoch\", \"epoch_loss\"])\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "    model = model.double()\n",
    "    # print(\"***************************start to train*****************************\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        train_each_batch_loss = []\n",
    "        train_each_batch_loss_total = 0\n",
    "        for data_pkg in train_loader:\n",
    "            x = data_pkg[0].double().to(device)\n",
    "            y = data_pkg[1]\n",
    "            y = y.double().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()   \n",
    "            \n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "        \n",
    "            loss.backward()                     \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_each_batch_loss.append(loss.detach().item())\n",
    "            train_each_batch_loss_total += loss.detach().item()\n",
    "    \n",
    "        model.eval() \n",
    "        \n",
    "        valid_each_batch_loss = []\n",
    "        valid_each_batch_loss_total = 0\n",
    "        for data_pkg in valid_loader:\n",
    "            x = data_pkg[0].double().to(device)\n",
    "            y = data_pkg[1]\n",
    "            y = y.double().to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "            # if epoch==10:\n",
    "            #     plt.plot(y[0][:, 1].detach().cpu(), label='y')\n",
    "            #     plt.plot(pred[0][:, 1].detach().cpu(), label='pred')\n",
    "            #     plt.legend()\n",
    "            #     plt.show()\n",
    "                \n",
    "            valid_each_batch_loss.append(loss.detach().item())\n",
    "            valid_each_batch_loss_total += loss.detach().item()\n",
    "            \n",
    "        current_lr = (optimizer.param_groups[0])['lr']\n",
    "        train_indicator = train_each_batch_loss_total/len(train_loader)\n",
    "        valid_indicator = valid_each_batch_loss_total/len(valid_loader)\n",
    "        schedulerRP.step(valid_indicator)\n",
    "#         schedulerEXP.step()\n",
    "#         schedulerSTEP.step()\n",
    "\n",
    "        \n",
    "                \n",
    "        # print(\"Epoch {}: LR: {:.8f}, Train Loss: {:.8f}, Valid Loss: {:.8f} for one layers\".format(epoch, current_lr, train_indicator, valid_indicator))\n",
    "        \n",
    "        if best_loss<valid_indicator: # loss不降反增\n",
    "            early_stop_count += 1\n",
    "        else:\n",
    "            if np.abs(best_loss-valid_indicator)>1e-4: # loss下降达到指标\n",
    "                best_loss = valid_indicator\n",
    "                early_stop_count = 0\n",
    "            else:\n",
    "                early_stop_count += 1 # loss下降但没有达到指标\n",
    "                \n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            break\n",
    "        if epoch>100 and (valid_indicator-train_indicator)>0.5:  # 防止过拟合\n",
    "            break\n",
    "        if epoch>100 and train_indicator>1:  # 防止收敛过慢\n",
    "            break\n",
    "\n",
    "        # wandb logging\n",
    "        ############################################################################################\n",
    "        wandb.log({'epoch':epoch, \n",
    "                   'lr':current_lr, \n",
    "                   'train_indicator':train_indicator, \n",
    "                   'valid_indicator':valid_indicator, \n",
    "                  })\n",
    "        ###########################################################################################\n",
    "    \n",
    "    return model, best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OBYgjCA-YwD"
   },
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdttVRkAfu2t",
    "outputId": "864a1214-e69e-4197-dda0-ce2f6fe07d3c"
   },
   "outputs": [],
   "source": [
    "def training():\n",
    "    # 初始化wandb\n",
    "    ############################################################################################\n",
    "    nowtime = datetime.datetime.now().strftime('%Y_%m_%d_%H%M%S')\n",
    "    wandb.init(\n",
    "      project='YOUR_PROJECT', \n",
    "      name=nowtime, \n",
    "      )\n",
    "    config = wandb.config\n",
    "    ############################################################################################\n",
    "    \n",
    "    # 初始化device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # 加载数据集\n",
    "    dataset, train_loader, valid_loader, test_loader = create_dataloader(config)\n",
    "    \n",
    "    # 创建模型保存目录\n",
    "    if os.path.exists(config['model_save_dir'])==False:\n",
    "        os.makedirs(config['model_save_dir'], exist_ok=True)\n",
    "\n",
    "    # 创建模型\n",
    "    model = LSMLSTM(config['seq_length'], 12, config['hidden_size'], config['soil_layer_num']).to(device) \n",
    "\n",
    "    # 模型训练\n",
    "    best_model, best_loss = trainer(train_loader, valid_loader, model, wandb, device)\n",
    "    \n",
    "    # 保存模型\n",
    "    if best_loss<0.5:\n",
    "        save_name = os.path.join(config['model_save_dir'], nowtime + '_STC.ckpt')\n",
    "        torch.save(best_model.state_dict(), save_name)\n",
    "        arti_code = wandb.Artifact('ipynb', type='code')\n",
    "        arti_code.add_file(os.path.join(config['root'], 'SURROGATE_TRAINING_WANDB.ipynb'))\n",
    "        arti_code.add_file(os.path.join(config['root'], 'LSMDataset.py'))\n",
    "        arti_code.add_file(os.path.join(config['root'], 'LSMLoss.py'))\n",
    "        arti_code.add_file(os.path.join(config['root'], 'LSMTransformer.py'))\n",
    "                                              \n",
    "        # arti_model = wandb.Artifact('model', type='model')\n",
    "        # arti_model.add_file(save_name)\n",
    "        wandb.log_artifact(arti_code)\n",
    "        # wandb.log_artifact(arti_model)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training(constant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='YOUR_PROJECT')\n",
    "print(sweep_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.agent(project='YOUR_PROJECT', sweep_id='l3kbhtzo', function=training, count=1)\n",
    "wandb.agent(sweep_id, training, count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML2022Spring - HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
