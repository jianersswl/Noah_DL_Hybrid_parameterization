{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igqIMEgu64-F"
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xybQNYCXYu13"
   },
   "outputs": [],
   "source": [
    "# Reading/Writing Data\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Pytorch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import gradcheck\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Self-Defined Package\n",
    "from PGDataset import PGDataset\n",
    "from PGNetwork import DynamicFCNetwork\n",
    "from LSMTransformer import LSMLSTM\n",
    "\n",
    "# 忽略 ParserWarning 警告\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.ParserWarning)\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pgkOh2e9UjE"
   },
   "source": [
    "# Configurations\n",
    "`config` contains hyper-parameters for training and the path to save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QoWPUahCtoT6"
   },
   "outputs": [],
   "source": [
    "constant_config = {\n",
    "    'seed': 11611801,      \n",
    "    'test_ratio': 0,\n",
    "    'valid_ratio': 0.2,   \n",
    "    'n_epochs': 1000,                \n",
    "    'train_batch_size': 128, \n",
    "    'valid_batch_size': 128,\n",
    "    \n",
    "    'learning_rate': 35e-4,\n",
    "    'step_size': 20,\n",
    "    'gamma': 0.2,\n",
    "    'weight_decay': 0.0025,\n",
    "    'warm_step': 11,\n",
    "    'early_stop': 50, \n",
    "    'hidden_size': 64,\n",
    "    'loss_decrease_threshold': 1e-6,\n",
    "    'param_factor': 0.001,\n",
    "    \n",
    "    # 目前只训练near sites的cmfd点\n",
    "    'standardization': True,\n",
    "    'soil_layer_num': [0, 4, 5, 6, 7],\n",
    "    'evaluate_layer': [0, 4, 5],\n",
    "    'var_param_list': ['MAXSMC'],\n",
    "    'seq_length': 365,\n",
    "    'output': 1,\n",
    "    'root': 'YOUR_ROOT\\\\Parameter_Generator',\n",
    "    'model_save_dir': 'YOUR_ROOT\\\\Parameter_Generator\\\\GENERATOR'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "sweep_config['parameters'] = {}\n",
    "\n",
    "# 常量型超参数\n",
    "sweep_config['parameters'].update({\n",
    "    'valid_batch_size': {'value': 128},\n",
    "    'seed': {'value': 11611801},      \n",
    "    'valid_ratio': {'value': 0.2}, \n",
    "    'n_epochs': {'value': 1000},\n",
    "    'soil_layer_num': {'value': [0, 4, 5, 6, 7]},\n",
    "    'evaluate_layer': {'value': [0, 4, 5]},\n",
    "    'seq_length': {'value': 365},\n",
    "    'output': {'value': 1},\n",
    "    'var_param_list': {'value': ['MAXSMC']},\n",
    "    'root': {'value': 'YOUR_ROOT\\\\Parameter_Generator'},\n",
    "    'model_save_dir': {'value': 'YOUR_ROOT\\\\Parameter_Generator\\\\GENERATOR'},\n",
    "})\n",
    "    \n",
    "# 离散型超参数\n",
    "sweep_config['parameters'].update({\n",
    "    'standardization': {\n",
    "        'values': [True, False]\n",
    "    },\n",
    "    'step_size': {\n",
    "        'values': [10, 20, 40]\n",
    "    },\n",
    "    'train_batch_size': {\n",
    "        'values': [32, 64, 128, 256]\n",
    "    },\n",
    "    'early_stop': {\n",
    "        'values': [50, 100, 150, 200]\n",
    "    },\n",
    "    'start_hidden_size': {\n",
    "        'values': [32, 64, 128, 256]\n",
    "    },\n",
    "    'end_hidden_size': {\n",
    "        'values': [8, 16, 32]\n",
    "    },\n",
    "})\n",
    "\n",
    "    \n",
    "# 连续型超参数\n",
    "sweep_config['parameters'].update({\n",
    "    'learning_rate': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-6,\n",
    "        'max': 5e-2\n",
    "      },\n",
    "    'weight_decay': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-4,\n",
    "        'max': 1e-1\n",
    "      },\n",
    "    'gamma': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-1,\n",
    "        'max': 8e-1,\n",
    "      },\n",
    "    'loss_decrease_threshold': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-6,\n",
    "        'max': 1e-4,\n",
    "    },\n",
    "    'param_factor': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-6,\n",
    "        'max': 1,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTAVqRfc2KK3"
   },
   "source": [
    "# Some Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RbrcpfYN2I-H"
   },
   "outputs": [],
   "source": [
    "def same_seed(seed): \n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrS-aJJh9XkW"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    gen_pkgs = np.asarray([gen_pkg for gen_pkg, _, _, _ in batch])\n",
    "    surr_pkgs = np.asarray([surr_pkg for _, surr_pkg, _, _ in batch])\n",
    "    label_pkgs = np.asarray([label_pkg for _, _, label_pkg, _ in batch])\n",
    "    meta_pkgs =  np.asarray([meta_pkg for _, _, _, meta_pkg in batch])\n",
    "    return torch.tensor(gen_pkgs, dtype=torch.double), torch.tensor(surr_pkgs, dtype=torch.double), torch.tensor(label_pkgs, dtype=torch.double), torch.tensor(meta_pkgs, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(config):    \n",
    "    # 创建数据集\n",
    "    dataset = PGDataset(config)\n",
    "    \n",
    "    # 计算训练集和测试集的长度\n",
    "    train_len = int(len(dataset) * (1-config['valid_ratio']))\n",
    "    valid_len = len(dataset) - train_len\n",
    "\n",
    "    # 使用 random_split 函数进行划分\n",
    "    train_dataset, valid_dataset = random_split(dataset, [train_len, valid_len])\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['train_batch_size'], drop_last=True, shuffle=True, pin_memory=False, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=config['valid_batch_size'], drop_last=False, shuffle=True, pin_memory=False, collate_fn=collate_fn)\n",
    "\n",
    "    print(len(dataset))\n",
    "    print('training size', len(train_dataset))\n",
    "    print('validing size', len(valid_dataset))\n",
    "    \n",
    "    return dataset, train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612\n",
      "training size 489\n",
      "validing size 123\n",
      "torch.Size([24])\n",
      "torch.Size([365, 12])\n",
      "torch.Size([365, 3])\n"
     ]
    }
   ],
   "source": [
    "dataset, train_loader, valid_loader = create_dataloader(constant_config)\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset[0][1].shape)\n",
    "print(dataset[0][2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer and Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_loader, valid_loader, surrogate, generator, device, config=None, wandb=None):\n",
    "    if wandb != None:\n",
    "        # 初始化config和criterion\n",
    "        config = wandb.config\n",
    "        \n",
    "    criterion =  nn.MSELoss() \n",
    "\n",
    "    #初始化optimizer和lr_scheduler\n",
    "    optimizer = torch.optim.Adam(generator.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay']) \n",
    "    schedulerRP = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=config['step_size'], factor=config['gamma'], min_lr=1e-8, threshold=config['loss_decrease_threshold'])\n",
    "#     schedulerEXP = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config['gamma'])\n",
    "#     schedulerSTEP = torch.optim.lr_scheduler.StepLR(optimizer, gamma=config['gamma'], step_size=config['step_size'])\n",
    "      \n",
    "    # table = wandb.Table(columns=[\"epoch\", \"epoch_loss\"])\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "    surrogate = surrogate.double()\n",
    "    generator = generator.double()\n",
    "    \n",
    "    # print(\"***************************start to train*****************************\")\n",
    "    columns = ['epoch', 'param']\n",
    "    param_table = wandb.Table(columns=columns)\n",
    "    for epoch in range(n_epochs):\n",
    "        generator.train()\n",
    "        train_each_batch_loss = []\n",
    "        train_each_batch_loss_total = 0\n",
    "        for data_pkg in train_loader:\n",
    "            gen_input = data_pkg[0].double().to(device)\n",
    "            surr_input = data_pkg[1].double().to(device)\n",
    "            label = data_pkg[2].double().to(device)\n",
    "#             print(gen_input.shape, surr_input.shape, label.shape)\n",
    "            optimizer.zero_grad()   \n",
    "            \n",
    "            param = generator(gen_input) * config['param_factor']\n",
    "            mask = torch.zeros_like(surr_input)\n",
    "            mask[:, :, 1] += param\n",
    "            surr_input = torch.add(surr_input, mask)\n",
    "            \n",
    "            pred = surrogate(surr_input)\n",
    "            loss = criterion(pred[:, :, :3], label)\n",
    "        \n",
    "            loss.backward()                     \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_each_batch_loss.append(loss.detach().item())\n",
    "            train_each_batch_loss_total += loss.detach().item()\n",
    "    \n",
    "        generator.eval() \n",
    "        valid_each_batch_loss = []\n",
    "        valid_each_batch_loss_total = 0\n",
    "        for data_pkg in valid_loader:\n",
    "            gen_input = data_pkg[0].double().to(device)\n",
    "            surr_input = data_pkg[1].double().to(device)\n",
    "            label = data_pkg[2].double().to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                gen_input = data_pkg[0].double().to(device)\n",
    "                surr_input = data_pkg[1].double().to(device)\n",
    "                label = data_pkg[2].double().to(device)\n",
    "\n",
    "                param = generator(gen_input) * config['param_factor']\n",
    "                mask = torch.zeros_like(surr_input)\n",
    "                mask[:, :, 1] += param\n",
    "                surr_input = torch.add(surr_input, mask)\n",
    "            \n",
    "                pred = surrogate(surr_input)\n",
    "                loss = criterion(pred[:, :, :3], label)\n",
    "            \n",
    "            if epoch%1==0:\n",
    "                for batch in range(param.shape[0]):\n",
    "                    param_table.add_data(epoch, param[batch].detach().item())       \n",
    "            valid_each_batch_loss.append(loss.detach().item())\n",
    "            valid_each_batch_loss_total += loss.detach().item()\n",
    "        \n",
    "        current_lr = (optimizer.param_groups[0])['lr']\n",
    "        train_indicator = train_each_batch_loss_total/len(train_loader)\n",
    "        valid_indicator = valid_each_batch_loss_total/len(valid_loader)\n",
    "        schedulerRP.step(valid_indicator)\n",
    "#         schedulerEXP.step()\n",
    "#         schedulerSTEP.step()\n",
    "\n",
    "        \n",
    "#         print(\"Epoch {}: LR: {:.8f}, Train Loss: {:.8f}, Valid Loss: {:.8f} for one layers\".format(epoch, current_lr, train_indicator, valid_indicator))\n",
    "        \n",
    "        if np.abs(best_loss-valid_indicator)<1e-4:\n",
    "            early_stop_count += 1\n",
    "        else:\n",
    "            best_loss = valid_indicator\n",
    "            early_stop_count = 0\n",
    "            best_generator = generator\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "#             wandb.log({\"termination_reason\": \"early_stop_count >= config['early_stop']\"})\n",
    "            break\n",
    "        if epoch>50 and (valid_indicator-train_indicator)>0.15:  # 防止过拟合\n",
    "#             wandb.log({\"termination_reason\": 'epoch>50 and (valid_indicator-train_indicator)>0.15'})\n",
    "            break\n",
    "        if epoch>50 and train_indicator>1.5:  # 防止收敛过慢\n",
    "#             wandb.log({\"termination_reason\": 'epoch>50 and train_indicator>1.5'})\n",
    "            break\n",
    "    \n",
    "        # wandb logging\n",
    "        ############################################################################################\n",
    "        wandb.log({'epoch': epoch, \n",
    "                   'lr': current_lr, \n",
    "                   'train_indicator': train_indicator, \n",
    "                   'valid_indicator': valid_indicator, \n",
    "                   'param_prediction': param_table\n",
    "                  })\n",
    "        ###########################################################################################\n",
    "\n",
    "    return best_generator, generator, best_loss, valid_indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OBYgjCA-YwD"
   },
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdttVRkAfu2t",
    "outputId": "864a1214-e69e-4197-dda0-ce2f6fe07d3c"
   },
   "outputs": [],
   "source": [
    "def training(config=None):\n",
    "    if config==None:\n",
    "        # 初始化wandb\n",
    "        ############################################################################################\n",
    "        nowtime = datetime.datetime.now().strftime('%Y_%m_%d_%H%M%S')\n",
    "        wandb.init(\n",
    "          project='YOUR_PROJECT', \n",
    "          name=nowtime, \n",
    "          )\n",
    "        config = wandb.config\n",
    "        ############################################################################################\n",
    "   \n",
    "    # 初始化device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "    \n",
    "    # 加载数据集\n",
    "    dataset, train_loader, valid_loader = create_dataloader(config)\n",
    "    \n",
    "    # 创建模型保存目录\n",
    "    if os.path.exists(config['model_save_dir'])==False:\n",
    "        os.makedirs(config['model_save_dir'], exist_ok=True)\n",
    "\n",
    "    # 创建模型\n",
    "    surrogate = LSMLSTM(config['seq_length'], 12, 64, len(config['soil_layer_num'])).to(device) \n",
    "    surrogate.load_state_dict(torch.load('YOUR_SURROGATE.ckpt'))\n",
    "    \n",
    "    hidden_sizes = [config['start_hidden_size'] // (2 ** i) for i in range(1+int(math.log2(config['start_hidden_size'] / config['end_hidden_size'])))]\n",
    "    print(hidden_sizes)\n",
    "    generator = DynamicFCNetwork(dataset[0][0].shape[0], hidden_sizes, 1).to(device) \n",
    "    print(generator)\n",
    "    \n",
    "    # 模型训练\n",
    "    best_generator, last_generator, best_loss, last_loss = trainer(train_loader, valid_loader, surrogate, generator, device, wandb=wandb, config=config)\n",
    "    \n",
    "    # 保存模型\n",
    "    if best_loss<0.015:\n",
    "        best_save_name = os.path.join(config['model_save_dir'], nowtime + '_best.ckpt')\n",
    "        last_save_name = os.path.join(config['model_save_dir'], nowtime + '_last.ckpt')\n",
    "        \n",
    "        torch.save(best_generator.state_dict(), best_save_name)\n",
    "        torch.save(last_generator.state_dict(), last_save_name)\n",
    "        \n",
    "        arti_code = wandb.Artifact('ipynb', type='code')\n",
    "        arti_code.add_file(os.path.join(config['root'], 'PG_TRAINING_UWC_WANDB.ipynb'))\n",
    "        arti_code.add_file(os.path.join(config['root'], 'PGDataset.py'))\n",
    "        arti_code.add_file(os.path.join(config['root'], 'PGNetwork.py'))\n",
    "                                              \n",
    "        # arti_model = wandb.Artifact('model', type='model')\n",
    "        # arti_model.add_file(save_name)\n",
    "        wandb.log_artifact(arti_code)\n",
    "        # wandb.log_artifact(arti_model)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training(constant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='YOUR_PROJECT')\n",
    "print(sweep_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wandb.agent(project='YOUR_PROJECT', sweep_id='l3kbhtzo', function=training, count=1)\n",
    "wandb.agent(sweep_id, training, count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML2022Spring - HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
